{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T04:07:43.873041Z",
     "start_time": "2021-01-26T04:07:42.138556Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from gensim import models\n",
    "from lib import weat\n",
    "import os\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "from scipy import stats\n",
    "import seaborn \n",
    "import fasttext\n",
    "import fasttext.util\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Glove Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T04:32:12.739496Z",
     "start_time": "2021-01-26T04:32:12.728808Z"
    }
   },
   "outputs": [],
   "source": [
    "# import and load glove model\n",
    "def loadGloveModel(file):\n",
    "    print(\"Loading glove model...\")\n",
    "    f = open(file,'r', encoding='utf-8')\n",
    "    gloveModel = {}\n",
    "    for line in f:\n",
    "        splitLines = line.split(' ')\n",
    "        word = splitLines[0]\n",
    "        wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
    "        gloveModel[word] = wordEmbedding\n",
    "    print(len(gloveModel),\" words loaded!\")\n",
    "    return gloveModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T04:35:20.881328Z",
     "start_time": "2021-01-26T04:32:12.930731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading glove model...\n",
      "2196017  words loaded!\n"
     ]
    }
   ],
   "source": [
    "# load Glove model\n",
    "glove = loadGloveModel('data/glove.840B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T04:05:41.647596Z",
     "start_time": "2021-01-26T04:05:41.630143Z"
    }
   },
   "source": [
    "## Load fastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T04:26:32.442853Z",
     "start_time": "2021-01-26T04:26:26.595141Z"
    }
   },
   "outputs": [],
   "source": [
    "ft = fasttext.load_model('data/cc.ht.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T04:26:32.535338Z",
     "start_time": "2021-01-26T04:26:32.450009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ft['dynamite'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T04:26:32.611671Z",
     "start_time": "2021-01-26T04:26:32.538701Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_tar_att_arrays(model, t1, a1, a2, algorithm, t2=None,):\n",
    "    if algorithm == 'weat':\n",
    "        tar1 = np.array([model[vector] for vector in t1])\n",
    "        tar2 = np.array([model[vector] for vector in t2])\n",
    "        att1 = np.array([model[vector] for vector in a1])\n",
    "        att2 = np.array([model[vector] for vector in a2])\n",
    "        \n",
    "        return tar1, tar2, att1, att2\n",
    "    \n",
    "    elif algorithm == 'wefat':\n",
    "        tar1 = np.array([model[vector] for vector in t1])\n",
    "        att1 = np.array([model[vector] for vector in a1])\n",
    "        att2 = np.array([model[vector] for vector in a2])\n",
    "        \n",
    "        return tar1, att1, att2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T04:26:32.743436Z",
     "start_time": "2021-01-26T04:26:32.614086Z"
    }
   },
   "outputs": [],
   "source": [
    "def normal_test(distr):\n",
    "    k2, p = stats.normaltest(distr)\n",
    "    alpha = 1e-3\n",
    "    if p < alpha: \n",
    "        return 'Yes'\n",
    "    else: return 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T04:38:13.647292Z",
     "start_time": "2021-01-26T04:38:13.613700Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_matrices(filepath, model, algorithm):\n",
    "    data_file = pd.read_csv(filepath, sep=',\\s*', engine='python',  header=None, index_col=0)\n",
    "    \n",
    "    if algorithm == 'weat':\n",
    "        # get targets and attribute labels\n",
    "        target_names = list(data_file.index)[:2]\n",
    "        attribute_names = list(data_file.index)[2:]\n",
    "\n",
    "        # get targets and attribute sets\n",
    "        targets = data_file.loc[target_names]\n",
    "        attributes = data_file.loc[attribute_names]\n",
    "\n",
    "        # get arrays, one for each set of target and attribute\n",
    "        tar1 = targets.loc[target_names[0]]\n",
    "        tar2 = targets.loc[target_names[1]]\n",
    "        att1 = attributes.loc[attribute_names[0]]\n",
    "        att2 = attributes.loc[attribute_names[1]]\n",
    "\n",
    "        # remove any NaN values that have been read due to mismatch of columns \n",
    "        tar1 = tar1[~pd.isna(tar1)]\n",
    "        tar2 = tar2[~pd.isna(tar2)]\n",
    "        att1 = att1[~pd.isna(att1)]\n",
    "        att2 = att2[~pd.isna(att2)]\n",
    "\n",
    "        # give numpy array of glove word embeddings for targets and attributes\n",
    "        tar1, tar2, att1, att2 = get_tar_att_arrays(model, tar1, att1, att2, algorithm, t2 = tar2, )\n",
    "\n",
    "        return target_names, attribute_names, tar1, tar2, att1, att2\n",
    "    \n",
    "    elif algorithm == 'wefat':\n",
    "        # get targets and attribute labels\n",
    "        target_names = list(data_file.index)[:1]\n",
    "        attribute_names = list(data_file.index)[1:]\n",
    "\n",
    "        # get targets and attribute sets\n",
    "        targets = data_file.loc[target_names]\n",
    "        attributes = data_file.loc[attribute_names]\n",
    "\n",
    "        # get arrays, one for each set of target and attribute\n",
    "        tar1 = targets.loc[target_names[0]]\n",
    "        att1 = attributes.loc[attribute_names[0]]\n",
    "        att2 = attributes.loc[attribute_names[1]]\n",
    "\n",
    "        # remove any NaN values that have been read due to mismatch of columns \n",
    "        tar1 = tar1[~pd.isna(tar1)]\n",
    "        att1 = att1[~pd.isna(att1)]\n",
    "        att2 = att2[~pd.isna(att2)]\n",
    "        \n",
    "        # give numpy array of glove word embeddings for targets and attributes\n",
    "        tar1, att1, att2 = get_tar_att_arrays(model, tar1, att1, att2, algorithm)\n",
    "\n",
    "        return target_names, attribute_names, tar1, att1, att2\n",
    "\n",
    "def output_values(filepath, model, algorithm): \n",
    "    # algorithm selection\n",
    "    if algorithm == 'weat':     \n",
    "        alg_object = weat.Weat()\n",
    "        \n",
    "        # retrieve target names and attributes to form the table\n",
    "        target_names, attribute_names, _, _, _, _ = get_matrices(filepath, model, algorithm)\n",
    "        # retrive the word embeddings for the targets and attributes\n",
    "        _, _, t1, t2, a1, a2 = get_matrices(filepath, model, algorithm)\n",
    "        # calculate the effect size \n",
    "        effect_size = alg_object.effect_size(t1, t2, a1, a2)\n",
    "        # calculate the p-value, test statistic, and permutations\n",
    "        p_val, test_stat, distr = alg_object.p_value(t1, t2, a1, a2)\n",
    "        \n",
    "        \n",
    "    elif algorithm =='wefat':\n",
    "        alg_object = weat.Wefat()\n",
    "        \n",
    "        # retrieve target names and attributes to form the table\n",
    "        target_names, attribute_names, _, _, _ = get_matrices(filepath, model, algorithm)\n",
    "        # retrive the word embeddings for the targets and attributes\n",
    "        _, _, targets, a1, a2 = get_matrices(filepath, model, algorithm)\n",
    "        \n",
    "        effect_size, p_val = list(), list()\n",
    "        for target in targets:\n",
    "            # calculate the effect size \n",
    "            eff_size = alg_object.effect_size(target, a1, a2)\n",
    "            effect_size.append(eff_size)\n",
    "            \n",
    "            # calculate the p-value, test statistic, and permutations\n",
    "            p_value, test_stat, distr = alg_object.p_value(target, a1, a2)\n",
    "            p_val.append(p_value)\n",
    "\n",
    "    \n",
    "    return target_names, attribute_names, effect_size, p_val\n",
    "\n",
    "def output_table(model, algorithm, directory=None, filepath=None):\n",
    "    print('Reading files...\\n')\n",
    "    \n",
    "    # if a directory is given to run the test on all files\n",
    "    if directory: \n",
    "        targets, attributes, effect_size, p_value = dict(), dict(), dict(), dict()\n",
    "        # loop over all files in the directory\n",
    "        for index, filename in enumerate(os.listdir(directory)):\n",
    "            if not filename.startswith('.'):  # ignore hidden files\n",
    "                filepath = directory + filename\n",
    "                print (filepath)\n",
    "                targets[index], attributes[index], effect_size[index], p_value[index] = \\\n",
    "                                            output_values(filepath, model, algorithm)\n",
    "        # create a dataframe with the targets, attributes, and effect size\n",
    "        output_df = pd.DataFrame(data = list(zip(targets.values(), attributes.values(), effect_size.values(), p_value.values())),\n",
    "                             columns = ['Targets', 'Attributes', 'Effect Size', 'P-Value'])\n",
    "#         output_df['Effect Size'] = output_df['Effect Size'].round(decimals=2)\n",
    "                \n",
    "    # if a specific filepath is given to the run a test only on that file          \n",
    "    elif filepath: \n",
    "        _, _, e_s, p_value = output_values(filepath, model, algorithm)\n",
    "        if algorithm == 'wefat':\n",
    "            target = ['mountain', 'movie', 'murderer']\n",
    "            output_df = pd.DataFrame(data = list(zip(target, e_s, p_value)), columns = ['Target', 'Effect Size', 'P-Value'])\n",
    "        elif algorithm =='weat':\n",
    "            output_df = pd.DataFrame(data = {'Effect Size': e_s, 'P-Value': p_value}, index=[0])\n",
    "        output_df['Effect Size'] = output_df['Effect Size'].round(decimals=2)\n",
    "\n",
    "    print('Finished.')\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T19:54:22.671528Z",
     "start_time": "2021-01-26T19:46:07.885060Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n",
      "\n",
      "targets_attributes_data/haitian-creole/instruments vs weapons.csv\n",
      "targets_attributes_data/haitian-creole/flowers vs insects.csv\n",
      "targets_attributes_data/haitian-creole/sanitation vs disease.csv\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "# haitian-creole weat test - flowers vs insects\n",
    "df_haitian = output_table(model=ft, directory='targets_attributes_data/haitian-creole/', algorithm='weat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T19:54:22.707387Z",
     "start_time": "2021-01-26T19:54:22.675433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Targets</th>\n",
       "      <th>Attributes</th>\n",
       "      <th>Effect Size</th>\n",
       "      <th>P-Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Enstriman, Zam]</td>\n",
       "      <td>[Pleasant, Dezagreyab]</td>\n",
       "      <td>0.538364</td>\n",
       "      <td>0.027461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Flè, Ensèk]</td>\n",
       "      <td>[Pleasant, Dezagreyab]</td>\n",
       "      <td>0.327377</td>\n",
       "      <td>0.234303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[sanitasyon, maladi]</td>\n",
       "      <td>[Pleasant, Dezagreyab]</td>\n",
       "      <td>0.140633</td>\n",
       "      <td>0.396739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Targets              Attributes  Effect Size   P-Value\n",
       "0      [Enstriman, Zam]  [Pleasant, Dezagreyab]     0.538364  0.027461\n",
       "1          [Flè, Ensèk]  [Pleasant, Dezagreyab]     0.327377  0.234303\n",
       "2  [sanitasyon, maladi]  [Pleasant, Dezagreyab]     0.140633  0.396739"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_haitian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-26T20:45:00.191790Z",
     "start_time": "2021-01-26T20:45:00.120655Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>',\n",
       " '.',\n",
       " 'nan',\n",
       " '\"',\n",
       " ',',\n",
       " 'yon',\n",
       " ':',\n",
       " \"'\",\n",
       " 'se',\n",
       " 'vil',\n",
       " ')',\n",
       " '(',\n",
       " 'eta',\n",
       " 'yo',\n",
       " 'peyi',\n",
       " 'moun',\n",
       " 'ki',\n",
       " 'li',\n",
       " 'te',\n",
       " 'ak',\n",
       " 'Etazini',\n",
       " 'de',\n",
       " 'pwovens',\n",
       " 'Kiba',\n",
       " 'lane',\n",
       " 'a',\n",
       " 'genyen',\n",
       " '}',\n",
       " 'pou',\n",
       " '_',\n",
       " 'nonm',\n",
       " 'Li',\n",
       " 'la',\n",
       " 'l',\n",
       " 'an',\n",
       " '2000',\n",
       " 'almanak',\n",
       " 'the',\n",
       " '/',\n",
       " 'Se',\n",
       " 'ant',\n",
       " 'Kategori',\n",
       " 'sistèm',\n",
       " 'Nan',\n",
       " 'natirèl',\n",
       " 'antye',\n",
       " 'sa',\n",
       " 'kèk',\n",
       " ';',\n",
       " 'jilyen',\n",
       " '\\\\',\n",
       " '%',\n",
       " 'La',\n",
       " 'pa',\n",
       " 'gen',\n",
       " 'and',\n",
       " '’',\n",
       " 'non',\n",
       " '-',\n",
       " 'to',\n",
       " 'New',\n",
       " 'ane',\n",
       " 'of',\n",
       " 'Gade',\n",
       " 'sou',\n",
       " 'd',\n",
       " '--',\n",
       " 'in',\n",
       " 'osi',\n",
       " 'del',\n",
       " 'fè',\n",
       " 'ayisyen',\n",
       " 'if',\n",
       " 'then',\n",
       " 'n',\n",
       " 'khi',\n",
       " 'is',\n",
       " 'ap',\n",
       " 'ː',\n",
       " 'Lonjitid',\n",
       " 'et',\n",
       " 'popilasyon',\n",
       " 'kòm',\n",
       " '2',\n",
       " 'itilize',\n",
       " 'El',\n",
       " 'Texas',\n",
       " 'local',\n",
       " 'Nou',\n",
       " 'ekri',\n",
       " 'e',\n",
       " 'York',\n",
       " 'ou',\n",
       " 'for',\n",
       " 'Latitid',\n",
       " 'avèk',\n",
       " 'ke',\n",
       " 'Yon',\n",
       " 'matematisyen',\n",
       " 'bagay',\n",
       " 'à',\n",
       " 'en',\n",
       " '1',\n",
       " 'konte',\n",
       " 'or',\n",
       " 'Habana',\n",
       " 'yòk',\n",
       " 'gregoryen',\n",
       " 'Pou',\n",
       " 'ka',\n",
       " 'epi',\n",
       " 'tout',\n",
       " '?',\n",
       " 'fèt',\n",
       " '2002',\n",
       " '2004',\n",
       " 'lòt',\n",
       " 'le',\n",
       " 's',\n",
       " 'soti',\n",
       " 'Rio',\n",
       " 'Pinar',\n",
       " 'Wisconsin',\n",
       " 'pi',\n",
       " '–',\n",
       " 'chif',\n",
       " '#',\n",
       " 'on',\n",
       " 'arab',\n",
       " 'Pennsilvani',\n",
       " 'lè',\n",
       " 'Matanzas',\n",
       " 'Las',\n",
       " 'atis',\n",
       " 'dinò',\n",
       " 'desimal',\n",
       " 'des',\n",
       " 'Ayiti',\n",
       " 'ekzadesimal',\n",
       " 'lang',\n",
       " 'dyodesimal',\n",
       " 'Image',\n",
       " 'y',\n",
       " 'Missouri',\n",
       " 'The',\n",
       " 'oswa',\n",
       " 'Ohio',\n",
       " 'right',\n",
       " '2005',\n",
       " '2001',\n",
       " 'tankou',\n",
       " '2003',\n",
       " 'franse',\n",
       " 'nou',\n",
       " '+',\n",
       " '..',\n",
       " 'anpil',\n",
       " 'kote',\n",
       " 'Popilasyon',\n",
       " 'pent',\n",
       " 'Minnesota',\n",
       " 'du',\n",
       " '^',\n",
       " 'Clara',\n",
       " 'Villa',\n",
       " 'di',\n",
       " 'Kawolin',\n",
       " '0',\n",
       " '!',\n",
       " 'not',\n",
       " 't',\n",
       " 'ti',\n",
       " 'return',\n",
       " 'tou',\n",
       " 'rejyon',\n",
       " 'oubyen',\n",
       " 'kapab',\n",
       " 'that',\n",
       " 'rele',\n",
       " 'you',\n",
       " 'Camaguey',\n",
       " 'Le',\n",
       " 'Dakota',\n",
       " 'San',\n",
       " 'les',\n",
       " 'Santiago',\n",
       " 'this',\n",
       " 'disid',\n",
       " 'm',\n",
       " 'un',\n",
       " 'men',\n",
       " 'p',\n",
       " '2007',\n",
       " 'Cuba',\n",
       " '2006',\n",
       " 'Franse',\n",
       " 'bay',\n",
       " 'be',\n",
       " '2008',\n",
       " 'Holguin',\n",
       " 'I',\n",
       " 'are',\n",
       " 'Ciego',\n",
       " 'function',\n",
       " 'menm',\n",
       " '2010',\n",
       " 'gwo',\n",
       " 'Oregon',\n",
       " 'pase',\n",
       " 'Oklahoma',\n",
       " 'with',\n",
       " 'jou',\n",
       " 'Tunas',\n",
       " 'Avila',\n",
       " 'km',\n",
       " 'lan',\n",
       " 'Dat',\n",
       " 'Angle',\n",
       " 'Granma',\n",
       " '3',\n",
       " 'sitye',\n",
       " 'premye',\n",
       " 'Lòt',\n",
       " '1905',\n",
       " '2009',\n",
       " 'Alman',\n",
       " 'plis',\n",
       " 'ekriven',\n",
       " 'Panyòl',\n",
       " 'Nebraska',\n",
       " 'Les',\n",
       " 'si',\n",
       " 'Jersey',\n",
       " 'L',\n",
       " 'reyalizatè',\n",
       " 'dans',\n",
       " 'Cienfuegos',\n",
       " 'oktal',\n",
       " 'it',\n",
       " 'from',\n",
       " 'san',\n",
       " 'èkh',\n",
       " '2011',\n",
       " 'left',\n",
       " 'at',\n",
       " '2014',\n",
       " 'UTC',\n",
       " 'error',\n",
       " 'ameriken',\n",
       " '2012',\n",
       " 'Los',\n",
       " 'est',\n",
       " '2016',\n",
       " 'el',\n",
       " 'pour',\n",
       " 'kreyòl',\n",
       " 'pati',\n",
       " 'date',\n",
       " 'amor',\n",
       " 'Sipèfisi',\n",
       " 'une',\n",
       " 'by',\n",
       " 'as',\n",
       " 'text',\n",
       " 'pran',\n",
       " '2013',\n",
       " 'tan',\n",
       " 'rive',\n",
       " 'Riemann',\n",
       " '«',\n",
       " '»',\n",
       " 'A',\n",
       " 'ha',\n",
       " 'byen',\n",
       " '²',\n",
       " 'travay',\n",
       " 'und',\n",
       " 'Bernhard',\n",
       " 'èh',\n",
       " 'me',\n",
       " 'kò',\n",
       " 'que',\n",
       " 'will',\n",
       " 'i',\n",
       " '~',\n",
       " 'Schwere',\n",
       " 'Elektricität',\n",
       " 'Magnetismus',\n",
       " 'value',\n",
       " 'set',\n",
       " 'Sancti',\n",
       " '4',\n",
       " 'vin',\n",
       " 'w',\n",
       " 'can',\n",
       " 'Non',\n",
       " 'Santa',\n",
       " 'dlo',\n",
       " 'Spiritus',\n",
       " 'Guantanamo',\n",
       " 'Washington',\n",
       " 'do',\n",
       " 'gwoup',\n",
       " 'syantifik',\n",
       " 'maladi',\n",
       " 'mete',\n",
       " 'mo',\n",
       " '1999',\n",
       " 'apre',\n",
       " 'pandan',\n",
       " '1998',\n",
       " 'vini',\n",
       " 'kapital',\n",
       " 'no',\n",
       " 'frac',\n",
       " 'jwenn',\n",
       " 'ale',\n",
       " '10',\n",
       " 'bon',\n",
       " 'Jean',\n",
       " 'konnen',\n",
       " 'vle',\n",
       " 'Tennessee',\n",
       " '2015',\n",
       " 'manje',\n",
       " 'have',\n",
       " 'té',\n",
       " 'fanm',\n",
       " 'Espay',\n",
       " 'style',\n",
       " 'k',\n",
       " 'meinme',\n",
       " 'mas',\n",
       " 'par',\n",
       " 'yeou',\n",
       " 'sur',\n",
       " '1996',\n",
       " 'seid',\n",
       " 'Ayisyen',\n",
       " 'pale',\n",
       " 'dwe',\n",
       " 'str',\n",
       " 'name',\n",
       " '>',\n",
       " 'page',\n",
       " 'City',\n",
       " 'kont',\n",
       " 'ta',\n",
       " 'liv',\n",
       " 'komin',\n",
       " 'year',\n",
       " 'Haïti',\n",
       " 'nil',\n",
       " 'This',\n",
       " 'IMDB',\n",
       " '1997',\n",
       " 'plizyè',\n",
       " 'Moun',\n",
       " '...',\n",
       " 'Mo',\n",
       " 'hou',\n",
       " 'Mississippi',\n",
       " 'Yo',\n",
       " 'bisektil',\n",
       " '1995',\n",
       " 'chak',\n",
       " 'il',\n",
       " '$',\n",
       " 'we',\n",
       " 'afriken',\n",
       " '12',\n",
       " 'your',\n",
       " 'María',\n",
       " 'tè',\n",
       " 'zha',\n",
       " '6',\n",
       " 'reprezante',\n",
       " 'khe',\n",
       " 'Sa',\n",
       " 'au',\n",
       " '”',\n",
       " 'kantite',\n",
       " 'plant',\n",
       " '15',\n",
       " 'wiki',\n",
       " 'jan',\n",
       " '“',\n",
       " '1992',\n",
       " 'toujou',\n",
       " 'list',\n",
       " 'youn',\n",
       " 'dyaspora',\n",
       " 'enfeksyon',\n",
       " '1993',\n",
       " 'al',\n",
       " 'aktè',\n",
       " 'rete',\n",
       " 'Utah',\n",
       " '1991',\n",
       " '1994',\n",
       " 'depi',\n",
       " 'tèt',\n",
       " 'apy',\n",
       " '1990',\n",
       " 'Ou',\n",
       " 'Nesans',\n",
       " 'has',\n",
       " 'kh',\n",
       " '5',\n",
       " 'yeo',\n",
       " 'string',\n",
       " 'wè',\n",
       " 'zòn',\n",
       " 'mwen',\n",
       " 'Khe',\n",
       " 'lèt',\n",
       " 'all',\n",
       " 'http',\n",
       " 'Juan',\n",
       " 'Haitian',\n",
       " 'fòme',\n",
       " 'An',\n",
       " 'language',\n",
       " 'match',\n",
       " 'mouri',\n",
       " 'pas',\n",
       " 'viv',\n",
       " 'Si',\n",
       " 'Hampshire',\n",
       " 'Vermont',\n",
       " 'lès',\n",
       " 'anba',\n",
       " 'politik',\n",
       " 'elseif',\n",
       " 'mwa',\n",
       " 'Fichye',\n",
       " '7',\n",
       " 'fanmi',\n",
       " '14',\n",
       " 'Vil',\n",
       " 'Vijini',\n",
       " 'message',\n",
       " '20',\n",
       " 'janvye',\n",
       " 'Paris',\n",
       " 'paske',\n",
       " 'link',\n",
       " 'son',\n",
       " 'If',\n",
       " 'one',\n",
       " 'Wikipedia',\n",
       " 'File',\n",
       " 'avril',\n",
       " 'Pòtoprens',\n",
       " 'table',\n",
       " 'parameter',\n",
       " 'De',\n",
       " 'José',\n",
       " 'angle',\n",
       " 'type',\n",
       " 'M',\n",
       " 'Sifas',\n",
       " 'ladan',\n",
       " 'anvan',\n",
       " 'jwe',\n",
       " 'mizik',\n",
       " 'fòm',\n",
       " 'jiyè',\n",
       " 'qui',\n",
       " 'discussion',\n",
       " 'times',\n",
       " 'true',\n",
       " 'bèt',\n",
       " 'Haiti',\n",
       " 'su',\n",
       " '1987',\n",
       " 'fòs',\n",
       " '0.00',\n",
       " 'yeon',\n",
       " 'Lè',\n",
       " 'atik',\n",
       " 'month',\n",
       " 'ansanm',\n",
       " '1986',\n",
       " 'kòmanse',\n",
       " 'out',\n",
       " 'laten',\n",
       " 'chimik',\n",
       " 'alman',\n",
       " 'Bolivar',\n",
       " 'gouvènman',\n",
       " '8',\n",
       " '18',\n",
       " 'Televisa',\n",
       " 'kay',\n",
       " 'CS1',\n",
       " 'kat',\n",
       " 'timoun',\n",
       " 'use',\n",
       " 'sèl',\n",
       " 'Gen',\n",
       " '1989',\n",
       " 'https',\n",
       " 'En',\n",
       " 'sèlman',\n",
       " 'ow',\n",
       " 'avec',\n",
       " 'Eta',\n",
       " 'Km',\n",
       " '1988',\n",
       " '2017',\n",
       " 'guen',\n",
       " 'args',\n",
       " 'other',\n",
       " 'Module',\n",
       " 'fwa',\n",
       " '100',\n",
       " 'abitan',\n",
       " 'fin',\n",
       " 'souvan',\n",
       " 'ann',\n",
       " 'diferan',\n",
       " 'gason',\n",
       " 'N',\n",
       " 'fevriye',\n",
       " 'code',\n",
       " 'lavi',\n",
       " 'number',\n",
       " '—',\n",
       " 'L.',\n",
       " 'prezidan',\n",
       " 'piti',\n",
       " 'fi',\n",
       " 'resevwa',\n",
       " 'Lake',\n",
       " 'kite',\n",
       " 'v',\n",
       " 'j',\n",
       " 'eleman',\n",
       " 'new',\n",
       " 'id',\n",
       " 'jen',\n",
       " 'ye',\n",
       " 'konsa',\n",
       " 'but',\n",
       " 'enpòtan',\n",
       " 'desanm',\n",
       " 'You',\n",
       " 'nenpòt',\n",
       " 'pye',\n",
       " 'Bondye',\n",
       " 'kreye',\n",
       " 'first',\n",
       " 'used',\n",
       " 'andedan',\n",
       " 'pâ',\n",
       " '13',\n",
       " '11',\n",
       " '•',\n",
       " 'kalite',\n",
       " 'devlope',\n",
       " '9',\n",
       " 'alfabèt',\n",
       " 'more',\n",
       " '22',\n",
       " 'sèvi',\n",
       " 'parameters',\n",
       " 'when',\n",
       " '1980',\n",
       " 'mizisyen',\n",
       " 'Un',\n",
       " 'also',\n",
       " 'title',\n",
       " 'll-l',\n",
       " '1985',\n",
       " 'selil',\n",
       " 'so',\n",
       " 'Carlos',\n",
       " '1979',\n",
       " 'dat',\n",
       " '17',\n",
       " '25',\n",
       " 'woman',\n",
       " 'pouvwa',\n",
       " '30',\n",
       " 'pitit',\n",
       " 'Creole',\n",
       " 'about',\n",
       " 'Imaj',\n",
       " 'lwa',\n",
       " 'septanm',\n",
       " 'mound',\n",
       " 'bezwen',\n",
       " 'los',\n",
       " '21',\n",
       " 'gaan',\n",
       " '1983',\n",
       " 'pèmèt',\n",
       " '16',\n",
       " 'ah',\n",
       " 'paj',\n",
       " 'pwodui',\n",
       " 'English',\n",
       " 'Sen',\n",
       " 'ni',\n",
       " 'lakòz',\n",
       " 'kwè',\n",
       " 'now',\n",
       " 'It',\n",
       " 'Men',\n",
       " 'touth',\n",
       " '19',\n",
       " 'Amerik',\n",
       " '1982',\n",
       " 'oktòb',\n",
       " 'mi',\n",
       " 'machin',\n",
       " 'help',\n",
       " 'mèt',\n",
       " 'twa',\n",
       " 'Chèf-lye',\n",
       " 'mal',\n",
       " 'Mi',\n",
       " 'end',\n",
       " 'ansyen',\n",
       " 'was',\n",
       " '&',\n",
       " 'article',\n",
       " '29',\n",
       " 'ISBN',\n",
       " 'montre',\n",
       " 'glif',\n",
       " 'chantè',\n",
       " 'langue',\n",
       " 'ede',\n",
       " 'moso',\n",
       " 'kenbe',\n",
       " 'nouvo',\n",
       " 'val',\n",
       " 'rapò',\n",
       " 'ba',\n",
       " 'ofisyèl',\n",
       " 'objè',\n",
       " 'Amor',\n",
       " 'sòti',\n",
       " 'Joseph',\n",
       " 'es',\n",
       " 'politisyen',\n",
       " 'fason',\n",
       " 'gran',\n",
       " 'Yeo',\n",
       " 'editor',\n",
       " 'mouvman',\n",
       " 'fe',\n",
       " 'mande',\n",
       " '1984',\n",
       " 'D',\n",
       " '1981',\n",
       " 'there',\n",
       " 'David',\n",
       " 'been',\n",
       " 'lajan',\n",
       " 'West',\n",
       " 'awondisman',\n",
       " 'o',\n",
       " 'pages',\n",
       " 'any',\n",
       " 'x',\n",
       " 'like',\n",
       " 'lekòl',\n",
       " 'Repiblik',\n",
       " 'manman',\n",
       " 'britanik',\n",
       " 'Saint',\n",
       " 'url',\n",
       " 'Afrik',\n",
       " 'Pierre',\n",
       " 'mond',\n",
       " 'Montana',\n",
       " 'pote',\n",
       " 'Il',\n",
       " 'sans',\n",
       " 'chanje',\n",
       " 'pral',\n",
       " 'make',\n",
       " 'which',\n",
       " 'López',\n",
       " 'òganis',\n",
       " 'enèji',\n",
       " 'Bolívar',\n",
       " 'renmen',\n",
       " 'sibstans',\n",
       " 'las',\n",
       " 'false',\n",
       " 'fizisyen',\n",
       " 'round',\n",
       " 'University',\n",
       " 'citation',\n",
       " 'long',\n",
       " 'pwoblèm',\n",
       " 'Antonio',\n",
       " 'only',\n",
       " 'Tout',\n",
       " '23',\n",
       " 'Wikimedia',\n",
       " 'vida',\n",
       " 'result',\n",
       " 'frame',\n",
       " 'depatman',\n",
       " 'Charles',\n",
       " 'anlè',\n",
       " 'konsidere',\n",
       " '24',\n",
       " 'pozisyon',\n",
       " 'fini',\n",
       " 'aux',\n",
       " '27',\n",
       " 'sifas',\n",
       " 'see',\n",
       " 'Please',\n",
       " 'Pari',\n",
       " 'Ewòp',\n",
       " '28',\n",
       " 'nesans',\n",
       " 'Kapital',\n",
       " 'template',\n",
       " 'add',\n",
       " 'istwa',\n",
       " 'names',\n",
       " 'imaj',\n",
       " 'ne',\n",
       " 'direksyon',\n",
       " 'Luis',\n",
       " 'milyon',\n",
       " 'too',\n",
       " 'Jacques',\n",
       " 'North',\n",
       " 'In',\n",
       " 'antre',\n",
       " 'teid',\n",
       " 'category',\n",
       " 'mache',\n",
       " 'last',\n",
       " '26',\n",
       " 'should',\n",
       " 'please',\n",
       " 'get',\n",
       " 'tete',\n",
       " 'Kèk',\n",
       " 'pah',\n",
       " 'Park',\n",
       " 'France',\n",
       " 'Ki',\n",
       " 'day',\n",
       " 'parèt',\n",
       " 'minis',\n",
       " 'wo',\n",
       " 'Soria',\n",
       " 'etid',\n",
       " 'format',\n",
       " 'dwa',\n",
       " 'plus',\n",
       " 'etidye',\n",
       " 'kontinye',\n",
       " 'image',\n",
       " 'zile',\n",
       " 'Ti',\n",
       " 'We',\n",
       " 'leta',\n",
       " 'novanm',\n",
       " 'solid',\n",
       " 'kominote',\n",
       " 'syèk',\n",
       " 'bouch',\n",
       " 'Nevada',\n",
       " 'militè',\n",
       " '200',\n",
       " 'tounen',\n",
       " 'kansè',\n",
       " 'prensipal',\n",
       " '1978',\n",
       " 'aksyon',\n",
       " 'cette',\n",
       " 'source',\n",
       " 'edit',\n",
       " 'laj',\n",
       " 'deplase',\n",
       " 'chanjman',\n",
       " 'pâr',\n",
       " 'gade',\n",
       " 'likid',\n",
       " 'here',\n",
       " 'sont',\n",
       " 'bwa',\n",
       " 'trè',\n",
       " 'Frans',\n",
       " 'Dos',\n",
       " '31',\n",
       " 'touye',\n",
       " 'kap',\n",
       " 'rezilta',\n",
       " '1975',\n",
       " 'French',\n",
       " 'syans',\n",
       " 'lòd',\n",
       " 'test',\n",
       " 'some',\n",
       " 'Inivèsite',\n",
       " 'je',\n",
       " 'selon',\n",
       " 'janm',\n",
       " 'rfloor',\n",
       " 'may',\n",
       " 'done',\n",
       " 'para',\n",
       " 'manm',\n",
       " 'To',\n",
       " 'bebe',\n",
       " 'qu',\n",
       " 'jeneral',\n",
       " 'lanmò',\n",
       " 'valid',\n",
       " 'wòch',\n",
       " '1977',\n",
       " 'nasyonal',\n",
       " 'sosyete',\n",
       " 'latè',\n",
       " 'mitan',\n",
       " 'egzanp',\n",
       " '1974',\n",
       " 'bò',\n",
       " 'Daniel',\n",
       " 'Pages',\n",
       " 'Pa',\n",
       " 'Jezi',\n",
       " 'Kanada',\n",
       " 'legliz',\n",
       " 'c',\n",
       " 'pèp',\n",
       " 'yonn',\n",
       " 'pwent',\n",
       " 'South',\n",
       " 'pairs',\n",
       " 'templates',\n",
       " 'chèf',\n",
       " 'patisipe',\n",
       " 'II',\n",
       " 'center',\n",
       " 'egziste',\n",
       " 'aktivite',\n",
       " 'trete',\n",
       " 'etc',\n",
       " 'tonumber',\n",
       " 'information',\n",
       " 'Premye',\n",
       " 'lis',\n",
       " 'Pedro',\n",
       " '0.4em',\n",
       " 'aktris',\n",
       " '…',\n",
       " 'koulè',\n",
       " 'modèl',\n",
       " 'VisualEditor',\n",
       " 'vivan',\n",
       " 'Ana',\n",
       " '1976',\n",
       " '1968',\n",
       " 'Venevisión',\n",
       " 'guenllein',\n",
       " 'pwen',\n",
       " 'ankò',\n",
       " 'Wyoming',\n",
       " 'baze',\n",
       " '1973',\n",
       " 'rantre',\n",
       " 'italyen',\n",
       " 'mennen',\n",
       " 'links',\n",
       " 'Apre',\n",
       " 'limyè',\n",
       " '1970',\n",
       " 'Wikipedya',\n",
       " 'Don',\n",
       " 'Ozetazini',\n",
       " 'sanble',\n",
       " 'Valley',\n",
       " 'Corazón',\n",
       " 'Miguel',\n",
       " 'konsène',\n",
       " 'Michel',\n",
       " 'grèk',\n",
       " 'Peyi',\n",
       " 'fou',\n",
       " 'Manuel',\n",
       " 'Batey',\n",
       " 'kòd',\n",
       " 'televizyon',\n",
       " 'part',\n",
       " 'S',\n",
       " 'sentòm',\n",
       " 'Venezyela',\n",
       " '1972',\n",
       " 'konn',\n",
       " 'mujer',\n",
       " 'my',\n",
       " 'fò',\n",
       " 'Gabriel',\n",
       " 'pwosesis',\n",
       " 'kounye',\n",
       " 'sous',\n",
       " 'endepandans',\n",
       " 'lame',\n",
       " 'move',\n",
       " 'Louis',\n",
       " 'peryòd',\n",
       " 'bakteri',\n",
       " 'fizik',\n",
       " 'solèy',\n",
       " 'planèt',\n",
       " 'ajan',\n",
       " 'deja',\n",
       " 'editors',\n",
       " 'tonbe',\n",
       " 'bot',\n",
       " '1969',\n",
       " 'Telemundo',\n",
       " 'voye',\n",
       " 'nivo',\n",
       " 'plas',\n",
       " '─',\n",
       " 'avek',\n",
       " 'images',\n",
       " 'Mwen',\n",
       " 'appou',\n",
       " 'Creek',\n",
       " 'enfòmasyon',\n",
       " 'Francisco',\n",
       " '1000',\n",
       " 'Isabel',\n",
       " 'Komin',\n",
       " 'John',\n",
       " 'anpeche',\n",
       " 'òganizasyon',\n",
       " 'TV',\n",
       " 'table.insert',\n",
       " 'Macintosh',\n",
       " 'konesans',\n",
       " 'precision',\n",
       " 'Karayib',\n",
       " 'Jan',\n",
       " 'Anpil',\n",
       " 'top',\n",
       " 'two',\n",
       " 'total',\n",
       " 'East',\n",
       " 'Rosa',\n",
       " 'ses',\n",
       " 'Me',\n",
       " 'panse',\n",
       " 'fonksyon',\n",
       " 'tap',\n",
       " 'articles',\n",
       " 'Wikidata',\n",
       " 'sum',\n",
       " 'dezyèm',\n",
       " 'Risi',\n",
       " 'tay',\n",
       " 'need',\n",
       " 'than',\n",
       " 'po',\n",
       " 'text-align',\n",
       " 'sant',\n",
       " 'cite',\n",
       " 'would',\n",
       " 'grandi',\n",
       " 'RCTV',\n",
       " 'tretman',\n",
       " 'medikaman',\n",
       " 'data',\n",
       " 'Liv',\n",
       " 'pèdi',\n",
       " 'edisyon',\n",
       " 'ogmante',\n",
       " 'Kreyòl',\n",
       " 'Lang',\n",
       " 'bar',\n",
       " 'fim',\n",
       " 'matematik',\n",
       " '1957',\n",
       " 'endikatif',\n",
       " 'available',\n",
       " 'Nasyonal',\n",
       " 'devlopman',\n",
       " 'Marie',\n",
       " 'errors',\n",
       " 'okenn',\n",
       " 'jis',\n",
       " 'kontinan',\n",
       " 'vignette',\n",
       " 'pat',\n",
       " ...]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-25T03:27:11.898584Z",
     "start_time": "2020-11-25T03:27:11.891050Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lib.weat' from '/Users/adimaini/Documents/GW/Machine Learning/Research/CODE/WEAT-WEFAT/lib/weat.py'>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(weat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T21:31:01.481108Z",
     "start_time": "2021-01-12T21:30:20.411083Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n",
      "\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "df_personalized_wefat = output_table(model=glove, filepath='personalized eval/wefat test.csv', algorithm='wefat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-25T03:27:39.311589Z",
     "start_time": "2020-11-25T03:27:35.892915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n",
      "\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "df_personalized_weat = output_table(model=glove, filepath='personalized eval/weat test.csv', algorithm='weat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-25T03:25:48.323145Z",
     "start_time": "2020-11-25T03:25:33.767004Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n",
      "\n",
      "targets_attributes_data/instruments vs weapons.csv\n",
      "targets_attributes_data/male vs female names.csv\n",
      "targets_attributes_data/science vs arts.csv\n",
      "targets_attributes_data/mental vs physical disease.csv\n",
      "targets_attributes_data/flowers vs insects.csv\n",
      "targets_attributes_data/young vs old peoples names.csv\n",
      "targets_attributes_data/math vs arts.csv\n",
      "targets_attributes_data/European-American vs African-American names 2.csv\n",
      "targets_attributes_data/European-American vs African-American names 3.csv\n",
      "targets_attributes_data/European-American vs African-American names 1.csv\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "df_glove = output_table(model=glove, directory='targets_attributes_data/', algorithm='weat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T21:31:01.491713Z",
     "start_time": "2021-01-12T21:31:01.483411Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Effect Size</th>\n",
       "      <th>P-Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aditya</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.269583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maini</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.419213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charu</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.730775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mishra</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.911692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mountain</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.001330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>movie</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.098460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>murderer</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>0.999305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Target  Effect Size   P-Value\n",
       "0    Aditya         0.18  0.269583\n",
       "1     Maini         0.06  0.419213\n",
       "2     Charu        -0.18  0.730775\n",
       "3    Mishra        -0.38  0.911692\n",
       "4  mountain         0.86  0.001330\n",
       "5     movie         0.37  0.098460\n",
       "6  murderer        -0.92  0.999305"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_personalized_wefat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-25T03:30:36.437976Z",
     "start_time": "2020-11-25T03:30:36.428920Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Effect Size</th>\n",
       "      <th>P-Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.17</td>\n",
       "      <td>0.121311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Effect Size   P-Value\n",
       "0         1.17  0.121311"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_personalized_weat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-25T02:52:53.744339Z",
     "start_time": "2020-11-25T02:52:53.727535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Targets</th>\n",
       "      <th>Attributes</th>\n",
       "      <th>Effect Size</th>\n",
       "      <th>P-Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Instruments, Weapons]</td>\n",
       "      <td>[Pleasant, Unpleasant]</td>\n",
       "      <td>1.53</td>\n",
       "      <td>4.284609e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Male names, Female names]</td>\n",
       "      <td>[Career, Family]</td>\n",
       "      <td>1.81</td>\n",
       "      <td>1.016746e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Science, Arts]</td>\n",
       "      <td>[Male terms, Female terms]</td>\n",
       "      <td>1.24</td>\n",
       "      <td>4.538500e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Mental disease, Physical disease]</td>\n",
       "      <td>[Temporary, Permanent]</td>\n",
       "      <td>1.38</td>\n",
       "      <td>6.993918e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Flowers, Insects]</td>\n",
       "      <td>[Pleasant, Unpleasant]</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.427875e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Young people’s names, Old people’s names]</td>\n",
       "      <td>[Pleasant, Unpleasant]</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1.467157e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[Math, Arts]</td>\n",
       "      <td>[Male terms, Female terms]</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.154923e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[European American names, African American names]</td>\n",
       "      <td>[Pleasant, Unpleasant]</td>\n",
       "      <td>1.50</td>\n",
       "      <td>3.707501e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[European American names, African American names]</td>\n",
       "      <td>[Pleasant, Unpleasant]</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.075243e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[European American names, African American names]</td>\n",
       "      <td>[Pleasant, Unpleasant]</td>\n",
       "      <td>1.41</td>\n",
       "      <td>4.246936e-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Targets  \\\n",
       "0                             [Instruments, Weapons]   \n",
       "1                         [Male names, Female names]   \n",
       "2                                    [Science, Arts]   \n",
       "3                 [Mental disease, Physical disease]   \n",
       "4                                 [Flowers, Insects]   \n",
       "5         [Young people’s names, Old people’s names]   \n",
       "6                                       [Math, Arts]   \n",
       "7  [European American names, African American names]   \n",
       "8  [European American names, African American names]   \n",
       "9  [European American names, African American names]   \n",
       "\n",
       "                   Attributes  Effect Size       P-Value  \n",
       "0      [Pleasant, Unpleasant]         1.53  4.284609e-06  \n",
       "1            [Career, Family]         1.81  1.016746e-04  \n",
       "2  [Male terms, Female terms]         1.24  4.538500e-03  \n",
       "3      [Temporary, Permanent]         1.38  6.993918e-03  \n",
       "4      [Pleasant, Unpleasant]         1.50  1.427875e-08  \n",
       "5      [Pleasant, Unpleasant]         1.21  1.467157e-02  \n",
       "6  [Male terms, Female terms]         1.06  1.154923e-02  \n",
       "7      [Pleasant, Unpleasant]         1.50  3.707501e-05  \n",
       "8      [Pleasant, Unpleasant]         1.28  1.075243e-04  \n",
       "9      [Pleasant, Unpleasant]         1.41  4.246936e-11  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-25T04:18:59.870622Z",
     "start_time": "2020-11-25T04:18:59.851326Z"
    }
   },
   "outputs": [],
   "source": [
    "df_glove.to_csv('output/weat_score_glove.csv')\n",
    "df_personalized_weat.to_csv('output/weat_score_personal.csv')\n",
    "df_personalized_wefat.to_csv('output/wefat_score_personal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python37364bit4db73d59933341feaa47a8db6a2db2c7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
